{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group Number: 1\n",
    "\n",
    "#Group Members:\n",
    "        # 1) Achal Agarwal : 2015B4A70436P\n",
    "        # 2) Vikram Waradpande : 2015B4A70454P\n",
    "        # 3) Mayank Bhutani: 2015B2A30836P\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import scipy.io as sc\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For one-hot encoding\n",
    "def one_hot(y_):\n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = np.max(y_) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29738, 65)\n",
      "[[4]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [3]\n",
      " [2]\n",
      " [1]]\n",
      "64\n",
      "Input to the CNN is of shape (28000, 64)\n",
      "Number of labels:  (7000, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/sklearn/utils/validation.py:590: DataConversionWarning: Data with input dtype int16 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------Training the CNN---------------\n",
      "Current Step: 5 ,Accuracy: 0.17485714 , Cost: 1.6828089\n",
      "Current Step: 10 ,Accuracy: 0.32828572 , Cost: 1.6046162\n",
      "Current Step: 15 ,Accuracy: 0.32828572 , Cost: 1.5767853\n",
      "Current Step: 20 ,Accuracy: 0.32828572 , Cost: 1.5737576\n",
      "Current Step: 25 ,Accuracy: 0.32828572 , Cost: 1.5712343\n",
      "Current Step: 30 ,Accuracy: 0.32828572 , Cost: 1.5698564\n",
      "Current Step: 35 ,Accuracy: 0.32828572 , Cost: 1.568653\n",
      "Current Step: 40 ,Accuracy: 0.32828572 , Cost: 1.5679067\n",
      "Current Step: 45 ,Accuracy: 0.32828572 , Cost: 1.5673351\n",
      "Current Step: 50 ,Accuracy: 0.32828572 , Cost: 1.5668802\n",
      "Current Step: 55 ,Accuracy: 0.32828572 , Cost: 1.5664759\n",
      "Current Step: 60 ,Accuracy: 0.32828572 , Cost: 1.5661372\n",
      "Current Step: 65 ,Accuracy: 0.32828572 , Cost: 1.5658565\n",
      "Current Step: 70 ,Accuracy: 0.32828572 , Cost: 1.5656319\n",
      "Current Step: 75 ,Accuracy: 0.32828572 , Cost: 1.5654649\n",
      "Current Step: 80 ,Accuracy: 0.32828572 , Cost: 1.5653387\n",
      "Current Step: 85 ,Accuracy: 0.32828572 , Cost: 1.5652571\n",
      "Current Step: 90 ,Accuracy: 0.32828572 , Cost: 1.5652001\n",
      "Current Step: 95 ,Accuracy: 0.32828572 , Cost: 1.5651541\n",
      "Current Step: 100 ,Accuracy: 0.32828572 , Cost: 1.5651356\n",
      "Current Step: 105 ,Accuracy: 0.32828572 , Cost: 1.5651112\n",
      "Current Step: 110 ,Accuracy: 0.32828572 , Cost: 1.5651151\n",
      "Current Step: 115 ,Accuracy: 0.32828572 , Cost: 1.5651311\n",
      "Current Step: 120 ,Accuracy: 0.32828572 , Cost: 1.565134\n",
      "Current Step: 125 ,Accuracy: 0.32828572 , Cost: 1.5651517\n",
      "Current Step: 130 ,Accuracy: 0.32828572 , Cost: 1.5651954\n",
      "Current Step: 135 ,Accuracy: 0.32828572 , Cost: 1.5652288\n",
      "Current Step: 140 ,Accuracy: 0.32828572 , Cost: 1.5652896\n",
      "Current Step: 145 ,Accuracy: 0.32828572 , Cost: 1.5653577\n",
      "Current Step: 150 ,Accuracy: 0.32828572 , Cost: 1.5654444\n",
      "Current Step: 155 ,Accuracy: 0.32828572 , Cost: 1.5655366\n",
      "Current Step: 160 ,Accuracy: 0.32828572 , Cost: 1.5656394\n",
      "Current Step: 165 ,Accuracy: 0.32828572 , Cost: 1.5657948\n",
      "Current Step: 170 ,Accuracy: 0.32828572 , Cost: 1.5659368\n",
      "Current Step: 175 ,Accuracy: 0.32828572 , Cost: 1.5660796\n",
      "Current Step: 180 ,Accuracy: 0.32828572 , Cost: 1.5662571\n",
      "Current Step: 185 ,Accuracy: 0.32828572 , Cost: 1.5664197\n",
      "Current Step: 190 ,Accuracy: 0.32828572 , Cost: 1.5666012\n",
      "Current Step: 195 ,Accuracy: 0.32828572 , Cost: 1.566735\n",
      "Current Step: 200 ,Accuracy: 0.32828572 , Cost: 1.5668557\n",
      "Current Step: 205 ,Accuracy: 0.32828572 , Cost: 1.5669727\n",
      "Current Step: 210 ,Accuracy: 0.32828572 , Cost: 1.5670372\n",
      "Current Step: 215 ,Accuracy: 0.32828572 , Cost: 1.5670999\n",
      "Current Step: 220 ,Accuracy: 0.32828572 , Cost: 1.5671687\n",
      "Current Step: 225 ,Accuracy: 0.32828572 , Cost: 1.5671884\n",
      "Current Step: 230 ,Accuracy: 0.32828572 , Cost: 1.5672265\n",
      "Current Step: 235 ,Accuracy: 0.32828572 , Cost: 1.5672367\n",
      "Current Step: 240 ,Accuracy: 0.32828572 , Cost: 1.5672258\n",
      "Current Step: 245 ,Accuracy: 0.32828572 , Cost: 1.5672383\n",
      "Current Step: 250 ,Accuracy: 0.32828572 , Cost: 1.5672324\n",
      "Current Step: 255 ,Accuracy: 0.32828572 , Cost: 1.5672104\n",
      "Current Step: 260 ,Accuracy: 0.32828572 , Cost: 1.567199\n",
      "Current Step: 265 ,Accuracy: 0.32828572 , Cost: 1.567178\n",
      "Current Step: 270 ,Accuracy: 0.32828572 , Cost: 1.5671632\n",
      "Current Step: 275 ,Accuracy: 0.32828572 , Cost: 1.5671351\n",
      "Current Step: 280 ,Accuracy: 0.32828572 , Cost: 1.567118\n",
      "Current Step: 285 ,Accuracy: 0.32828572 , Cost: 1.5671175\n",
      "Current Step: 290 ,Accuracy: 0.32828572 , Cost: 1.5671078\n",
      "Current Step: 295 ,Accuracy: 0.32828572 , Cost: 1.5670903\n",
      "Current Step: 300 ,Accuracy: 0.32828572 , Cost: 1.567097\n",
      "Current Step: 305 ,Accuracy: 0.32828572 , Cost: 1.5670549\n",
      "Current Step: 310 ,Accuracy: 0.32828572 , Cost: 1.5670637\n",
      "Current Step: 315 ,Accuracy: 0.32828572 , Cost: 1.5670449\n",
      "Current Step: 320 ,Accuracy: 0.32828572 , Cost: 1.5670409\n",
      "Current Step: 325 ,Accuracy: 0.32828572 , Cost: 1.5670159\n",
      "Current Step: 330 ,Accuracy: 0.32828572 , Cost: 1.5670059\n",
      "Current Step: 335 ,Accuracy: 0.32828572 , Cost: 1.5670034\n",
      "Current Step: 340 ,Accuracy: 0.32828572 , Cost: 1.5670013\n",
      "Current Step: 345 ,Accuracy: 0.32828572 , Cost: 1.5670078\n",
      "Current Step: 350 ,Accuracy: 0.32828572 , Cost: 1.5669906\n",
      "Current Step: 355 ,Accuracy: 0.32828572 , Cost: 1.566987\n",
      "Current Step: 360 ,Accuracy: 0.32828572 , Cost: 1.5669849\n",
      "Current Step: 365 ,Accuracy: 0.32828572 , Cost: 1.566975\n",
      "Current Step: 370 ,Accuracy: 0.32828572 , Cost: 1.5669862\n",
      "Current Step: 375 ,Accuracy: 0.32828572 , Cost: 1.5669733\n",
      "Current Step: 380 ,Accuracy: 0.32828572 , Cost: 1.5669657\n",
      "Current Step: 385 ,Accuracy: 0.32828572 , Cost: 1.5669684\n",
      "Current Step: 390 ,Accuracy: 0.32828572 , Cost: 1.566965\n",
      "Current Step: 395 ,Accuracy: 0.32828572 , Cost: 1.5669659\n",
      "Current Step: 400 ,Accuracy: 0.32828572 , Cost: 1.5669774\n",
      "Current Step: 405 ,Accuracy: 0.32828572 , Cost: 1.5669644\n",
      "Current Step: 410 ,Accuracy: 0.32828572 , Cost: 1.5669824\n",
      "Current Step: 415 ,Accuracy: 0.32828572 , Cost: 1.5669553\n",
      "Current Step: 420 ,Accuracy: 0.32828572 , Cost: 1.5669582\n",
      "Current Step: 425 ,Accuracy: 0.32828572 , Cost: 1.5669702\n",
      "Current Step: 430 ,Accuracy: 0.32828572 , Cost: 1.5669684\n",
      "Current Step: 435 ,Accuracy: 0.32828572 , Cost: 1.5669526\n",
      "Current Step: 440 ,Accuracy: 0.32828572 , Cost: 1.5669571\n",
      "Current Step: 445 ,Accuracy: 0.32828572 , Cost: 1.566952\n",
      "Current Step: 450 ,Accuracy: 0.32828572 , Cost: 1.5669616\n",
      "Current Step: 455 ,Accuracy: 0.32828572 , Cost: 1.566959\n",
      "Current Step: 460 ,Accuracy: 0.32828572 , Cost: 1.5669551\n",
      "Current Step: 465 ,Accuracy: 0.32828572 , Cost: 1.5669619\n",
      "Current Step: 470 ,Accuracy: 0.32828572 , Cost: 1.5669601\n",
      "Current Step: 475 ,Accuracy: 0.32828572 , Cost: 1.5669565\n",
      "Current Step: 480 ,Accuracy: 0.32828572 , Cost: 1.5669596\n",
      "Current Step: 485 ,Accuracy: 0.32828572 , Cost: 1.5669643\n",
      "Current Step: 490 ,Accuracy: 0.32828572 , Cost: 1.566946\n",
      "Current Step: 495 ,Accuracy: 0.32828572 , Cost: 1.5669562\n",
      "Current Step: 500 ,Accuracy: 0.32828572 , Cost: 1.5669435\n",
      "Current Step: 505 ,Accuracy: 0.32828572 , Cost: 1.5669425\n",
      "Current Step: 510 ,Accuracy: 0.32828572 , Cost: 1.5669507\n",
      "Current Step: 515 ,Accuracy: 0.32828572 , Cost: 1.5669581\n",
      "Current Step: 520 ,Accuracy: 0.32828572 , Cost: 1.5669546\n",
      "Current Step: 525 ,Accuracy: 0.32828572 , Cost: 1.5669644\n",
      "Current Step: 530 ,Accuracy: 0.32828572 , Cost: 1.5669514\n",
      "Current Step: 535 ,Accuracy: 0.32828572 , Cost: 1.5669712\n",
      "Current Step: 540 ,Accuracy: 0.32828572 , Cost: 1.566958\n",
      "Current Step: 545 ,Accuracy: 0.32828572 , Cost: 1.5669615\n",
      "Current Step: 550 ,Accuracy: 0.32828572 , Cost: 1.5669506\n",
      "Current Step: 555 ,Accuracy: 0.32828572 , Cost: 1.5669484\n",
      "Current Step: 560 ,Accuracy: 0.32828572 , Cost: 1.5669507\n",
      "Current Step: 565 ,Accuracy: 0.32828572 , Cost: 1.5669631\n",
      "Current Step: 570 ,Accuracy: 0.32828572 , Cost: 1.5669552\n",
      "Current Step: 575 ,Accuracy: 0.32828572 , Cost: 1.5669446\n",
      "Current Step: 580 ,Accuracy: 0.32828572 , Cost: 1.5669608\n",
      "Current Step: 585 ,Accuracy: 0.32828572 , Cost: 1.566952\n",
      "Current Step: 590 ,Accuracy: 0.32828572 , Cost: 1.566949\n",
      "Current Step: 595 ,Accuracy: 0.32828572 , Cost: 1.5669413\n",
      "Current Step: 600 ,Accuracy: 0.32828572 , Cost: 1.5669436\n",
      "Current Step: 605 ,Accuracy: 0.32828572 , Cost: 1.5669397\n",
      "Current Step: 610 ,Accuracy: 0.32828572 , Cost: 1.5669379\n",
      "Current Step: 615 ,Accuracy: 0.32828572 , Cost: 1.5669478\n",
      "Current Step: 620 ,Accuracy: 0.32828572 , Cost: 1.5669539\n",
      "Current Step: 625 ,Accuracy: 0.32828572 , Cost: 1.5669502\n",
      "Current Step: 630 ,Accuracy: 0.32828572 , Cost: 1.5669395\n",
      "Current Step: 635 ,Accuracy: 0.32828572 , Cost: 1.5669299\n",
      "Current Step: 640 ,Accuracy: 0.32828572 , Cost: 1.5669464\n",
      "Current Step: 645 ,Accuracy: 0.32828572 , Cost: 1.5669545\n",
      "Current Step: 650 ,Accuracy: 0.32828572 , Cost: 1.5669627\n",
      "Current Step: 655 ,Accuracy: 0.32828572 , Cost: 1.5669563\n",
      "Current Step: 660 ,Accuracy: 0.32828572 , Cost: 1.5669544\n",
      "Current Step: 665 ,Accuracy: 0.32828572 , Cost: 1.5669448\n",
      "Current Step: 670 ,Accuracy: 0.32828572 , Cost: 1.5669434\n",
      "Current Step: 675 ,Accuracy: 0.32828572 , Cost: 1.56694\n",
      "Current Step: 680 ,Accuracy: 0.32828572 , Cost: 1.5669427\n",
      "Current Step: 685 ,Accuracy: 0.32828572 , Cost: 1.5669537\n",
      "Current Step: 690 ,Accuracy: 0.32828572 , Cost: 1.5669501\n",
      "Current Step: 695 ,Accuracy: 0.32828572 , Cost: 1.5669514\n",
      "Current Step: 700 ,Accuracy: 0.32828572 , Cost: 1.5669364\n",
      "Current Step: 705 ,Accuracy: 0.32828572 , Cost: 1.5669469\n",
      "Current Step: 710 ,Accuracy: 0.32828572 , Cost: 1.5669448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 715 ,Accuracy: 0.32828572 , Cost: 1.5669272\n",
      "Current Step: 720 ,Accuracy: 0.32828572 , Cost: 1.5669346\n",
      "Current Step: 725 ,Accuracy: 0.32828572 , Cost: 1.5669327\n",
      "Current Step: 730 ,Accuracy: 0.32828572 , Cost: 1.5669333\n",
      "Current Step: 735 ,Accuracy: 0.32828572 , Cost: 1.5669262\n",
      "Current Step: 740 ,Accuracy: 0.32828572 , Cost: 1.5669262\n",
      "Current Step: 745 ,Accuracy: 0.32828572 , Cost: 1.5669259\n",
      "Current Step: 750 ,Accuracy: 0.32828572 , Cost: 1.5669245\n",
      "Current Step: 755 ,Accuracy: 0.32828572 , Cost: 1.5669242\n",
      "Current Step: 760 ,Accuracy: 0.32828572 , Cost: 1.5669314\n",
      "Current Step: 765 ,Accuracy: 0.32828572 , Cost: 1.566932\n",
      "Current Step: 770 ,Accuracy: 0.32828572 , Cost: 1.5669347\n",
      "Current Step: 775 ,Accuracy: 0.32828572 , Cost: 1.5669345\n",
      "Current Step: 780 ,Accuracy: 0.32828572 , Cost: 1.5669346\n",
      "Current Step: 785 ,Accuracy: 0.32828572 , Cost: 1.5669405\n",
      "Current Step: 790 ,Accuracy: 0.32828572 , Cost: 1.5669405\n",
      "Current Step: 795 ,Accuracy: 0.32828572 , Cost: 1.5669404\n",
      "Current Step: 800 ,Accuracy: 0.32828572 , Cost: 1.5669408\n",
      "Current Step: 805 ,Accuracy: 0.32828572 , Cost: 1.5669426\n",
      "Current Step: 810 ,Accuracy: 0.32828572 , Cost: 1.5669281\n",
      "Current Step: 815 ,Accuracy: 0.32828572 , Cost: 1.5669264\n",
      "Current Step: 820 ,Accuracy: 0.32828572 , Cost: 1.5669281\n",
      "Current Step: 825 ,Accuracy: 0.32828572 , Cost: 1.5669421\n",
      "Current Step: 830 ,Accuracy: 0.32828572 , Cost: 1.566942\n",
      "Current Step: 835 ,Accuracy: 0.32828572 , Cost: 1.5669421\n",
      "Current Step: 840 ,Accuracy: 0.32828572 , Cost: 1.5669422\n",
      "Current Step: 845 ,Accuracy: 0.32828572 , Cost: 1.5669423\n",
      "Current Step: 850 ,Accuracy: 0.32828572 , Cost: 1.5669409\n",
      "Current Step: 855 ,Accuracy: 0.32828572 , Cost: 1.5669408\n",
      "Current Step: 860 ,Accuracy: 0.32828572 , Cost: 1.5669428\n",
      "Current Step: 865 ,Accuracy: 0.32828572 , Cost: 1.566943\n",
      "Current Step: 870 ,Accuracy: 0.32828572 , Cost: 1.5669425\n",
      "Current Step: 875 ,Accuracy: 0.32828572 , Cost: 1.5669428\n",
      "Current Step: 880 ,Accuracy: 0.32828572 , Cost: 1.566943\n",
      "Current Step: 885 ,Accuracy: 0.32828572 , Cost: 1.5669428\n",
      "Current Step: 890 ,Accuracy: 0.32828572 , Cost: 1.5669423\n",
      "Current Step: 895 ,Accuracy: 0.32828572 , Cost: 1.5669422\n",
      "Current Step: 900 ,Accuracy: 0.32828572 , Cost: 1.5669423\n",
      "Current Step: 905 ,Accuracy: 0.32828572 , Cost: 1.5669422\n",
      "Current Step: 910 ,Accuracy: 0.32828572 , Cost: 1.5669422\n",
      "Current Step: 915 ,Accuracy: 0.32828572 , Cost: 1.5669421\n",
      "Current Step: 920 ,Accuracy: 0.32828572 , Cost: 1.5669422\n",
      "Current Step: 925 ,Accuracy: 0.32828572 , Cost: 1.5669315\n",
      "Current Step: 930 ,Accuracy: 0.32828572 , Cost: 1.5669442\n",
      "Current Step: 935 ,Accuracy: 0.32828572 , Cost: 1.566944\n",
      "Current Step: 940 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 945 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 950 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 955 ,Accuracy: 0.32828572 , Cost: 1.5669332\n",
      "Current Step: 960 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 965 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 970 ,Accuracy: 0.32828572 , Cost: 1.5669328\n",
      "Current Step: 975 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 980 ,Accuracy: 0.32828572 , Cost: 1.5669333\n",
      "Current Step: 985 ,Accuracy: 0.32828572 , Cost: 1.5669335\n",
      "Current Step: 990 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 995 ,Accuracy: 0.32828572 , Cost: 1.5669329\n",
      "Current Step: 1000 ,Accuracy: 0.32828572 , Cost: 1.5669335\n",
      "Current Step: 1005 ,Accuracy: 0.32828572 , Cost: 1.5669333\n",
      "Current Step: 1010 ,Accuracy: 0.32828572 , Cost: 1.5669329\n",
      "Current Step: 1015 ,Accuracy: 0.32828572 , Cost: 1.5669333\n",
      "Current Step: 1020 ,Accuracy: 0.32828572 , Cost: 1.5669329\n",
      "Current Step: 1025 ,Accuracy: 0.32828572 , Cost: 1.5669329\n",
      "Current Step: 1030 ,Accuracy: 0.32828572 , Cost: 1.5669329\n",
      "Current Step: 1035 ,Accuracy: 0.32828572 , Cost: 1.566933\n",
      "Current Step: 1040 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 1045 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1050 ,Accuracy: 0.32828572 , Cost: 1.5669334\n",
      "Current Step: 1055 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1060 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1065 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1070 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1075 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1080 ,Accuracy: 0.32828572 , Cost: 1.5669312\n",
      "Current Step: 1085 ,Accuracy: 0.32828572 , Cost: 1.5669311\n",
      "Current Step: 1090 ,Accuracy: 0.32828572 , Cost: 1.5669321\n",
      "Current Step: 1095 ,Accuracy: 0.32828572 , Cost: 1.5669354\n",
      "Current Step: 1100 ,Accuracy: 0.32828572 , Cost: 1.5669323\n",
      "Current Step: 1105 ,Accuracy: 0.32828572 , Cost: 1.5669407\n",
      "Current Step: 1110 ,Accuracy: 0.32828572 , Cost: 1.5669323\n",
      "Current Step: 1115 ,Accuracy: 0.32828572 , Cost: 1.5669321\n",
      "Current Step: 1120 ,Accuracy: 0.32828572 , Cost: 1.5669402\n",
      "Current Step: 1125 ,Accuracy: 0.32828572 , Cost: 1.5669318\n",
      "Current Step: 1130 ,Accuracy: 0.32828572 , Cost: 1.5669398\n",
      "Current Step: 1135 ,Accuracy: 0.32828572 , Cost: 1.5669315\n",
      "Current Step: 1140 ,Accuracy: 0.32828572 , Cost: 1.5669316\n",
      "Current Step: 1145 ,Accuracy: 0.32828572 , Cost: 1.5669398\n",
      "Current Step: 1150 ,Accuracy: 0.32828572 , Cost: 1.56694\n",
      "Current Step: 1155 ,Accuracy: 0.32828572 , Cost: 1.5669316\n",
      "Current Step: 1160 ,Accuracy: 0.32828572 , Cost: 1.5669404\n",
      "Current Step: 1165 ,Accuracy: 0.32828572 , Cost: 1.5669316\n",
      "Current Step: 1170 ,Accuracy: 0.32828572 , Cost: 1.5669361\n",
      "Current Step: 1175 ,Accuracy: 0.32828572 , Cost: 1.5669434\n",
      "Current Step: 1180 ,Accuracy: 0.32828572 , Cost: 1.566935\n",
      "Current Step: 1185 ,Accuracy: 0.32828572 , Cost: 1.5669361\n",
      "Current Step: 1190 ,Accuracy: 0.32828572 , Cost: 1.5669351\n",
      "Current Step: 1195 ,Accuracy: 0.32828572 , Cost: 1.5669357\n",
      "Current Step: 1200 ,Accuracy: 0.32828572 , Cost: 1.5669357\n",
      "Current Step: 1205 ,Accuracy: 0.32828572 , Cost: 1.5669273\n",
      "Current Step: 1210 ,Accuracy: 0.32828572 , Cost: 1.566934\n",
      "Current Step: 1215 ,Accuracy: 0.32828572 , Cost: 1.566934\n",
      "Current Step: 1220 ,Accuracy: 0.32828572 , Cost: 1.566929\n",
      "Current Step: 1225 ,Accuracy: 0.32828572 , Cost: 1.5669335\n",
      "Current Step: 1230 ,Accuracy: 0.32828572 , Cost: 1.5669338\n",
      "Current Step: 1235 ,Accuracy: 0.32828572 , Cost: 1.5669341\n",
      "Current Step: 1240 ,Accuracy: 0.32828572 , Cost: 1.566934\n",
      "Current Step: 1245 ,Accuracy: 0.32828572 , Cost: 1.5669341\n",
      "Current Step: 1250 ,Accuracy: 0.32828572 , Cost: 1.5669348\n",
      "Current Step: 1255 ,Accuracy: 0.32828572 , Cost: 1.5669267\n",
      "Current Step: 1260 ,Accuracy: 0.32828572 , Cost: 1.5669347\n",
      "Current Step: 1265 ,Accuracy: 0.32828572 , Cost: 1.5669347\n",
      "Current Step: 1270 ,Accuracy: 0.32828572 , Cost: 1.5669347\n",
      "Current Step: 1275 ,Accuracy: 0.32828572 , Cost: 1.5669343\n",
      "Current Step: 1280 ,Accuracy: 0.32828572 , Cost: 1.5669345\n",
      "Current Step: 1285 ,Accuracy: 0.32828572 , Cost: 1.5669345\n",
      "Current Step: 1290 ,Accuracy: 0.32828572 , Cost: 1.5669343\n",
      "Current Step: 1295 ,Accuracy: 0.32828572 , Cost: 1.5669345\n",
      "Current Step: 1300 ,Accuracy: 0.32828572 , Cost: 1.5669343\n",
      "Current Step: 1305 ,Accuracy: 0.32828572 , Cost: 1.5669345\n",
      "Current Step: 1310 ,Accuracy: 0.32828572 , Cost: 1.5669491\n",
      "Current Step: 1315 ,Accuracy: 0.32828572 , Cost: 1.5669491\n",
      "Current Step: 1320 ,Accuracy: 0.32828572 , Cost: 1.5669471\n",
      "Current Step: 1325 ,Accuracy: 0.32828572 , Cost: 1.5669471\n",
      "Current Step: 1330 ,Accuracy: 0.32828572 , Cost: 1.5669491\n",
      "Current Step: 1335 ,Accuracy: 0.32828572 , Cost: 1.5669472\n",
      "Current Step: 1340 ,Accuracy: 0.32828572 , Cost: 1.5669472\n",
      "Current Step: 1345 ,Accuracy: 0.32828572 , Cost: 1.5669413\n",
      "Current Step: 1350 ,Accuracy: 0.32828572 , Cost: 1.5669403\n",
      "Current Step: 1355 ,Accuracy: 0.32828572 , Cost: 1.5669403\n",
      "Current Step: 1360 ,Accuracy: 0.32828572 , Cost: 1.5669402\n",
      "Current Step: 1365 ,Accuracy: 0.32828572 , Cost: 1.5669403\n",
      "Current Step: 1370 ,Accuracy: 0.32828572 , Cost: 1.5669403\n",
      "Current Step: 1375 ,Accuracy: 0.32828572 , Cost: 1.5669411\n",
      "Current Step: 1380 ,Accuracy: 0.32828572 , Cost: 1.56694\n",
      "Current Step: 1385 ,Accuracy: 0.32828572 , Cost: 1.5669413\n",
      "Current Step: 1390 ,Accuracy: 0.32828572 , Cost: 1.5669409\n",
      "Current Step: 1395 ,Accuracy: 0.32828572 , Cost: 1.5669411\n",
      "Current Step: 1400 ,Accuracy: 0.32828572 , Cost: 1.5669408\n",
      "Current Step: 1405 ,Accuracy: 0.32828572 , Cost: 1.5669408\n",
      "Current Step: 1410 ,Accuracy: 0.32828572 , Cost: 1.5669409\n",
      "Current Step: 1415 ,Accuracy: 0.32828572 , Cost: 1.5669405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Step: 1420 ,Accuracy: 0.32828572 , Cost: 1.5669405\n",
      "Current Step: 1425 ,Accuracy: 0.32828572 , Cost: 1.5669407\n",
      "Current Step: 1430 ,Accuracy: 0.32828572 , Cost: 1.5669407\n",
      "Current Step: 1435 ,Accuracy: 0.32828572 , Cost: 1.5669407\n",
      "Current Step: 1440 ,Accuracy: 0.32828572 , Cost: 1.5669407\n",
      "Current Step: 1445 ,Accuracy: 0.32828572 , Cost: 1.5669441\n",
      "Current Step: 1450 ,Accuracy: 0.32828572 , Cost: 1.5669441\n",
      "Current Step: 1455 ,Accuracy: 0.32828572 , Cost: 1.5669441\n",
      "Current Step: 1460 ,Accuracy: 0.32828572 , Cost: 1.5669441\n",
      "Current Step: 1465 ,Accuracy: 0.32828572 , Cost: 1.5669441\n",
      "Current Step: 1470 ,Accuracy: 0.32828572 , Cost: 1.5669442\n",
      "Current Step: 1475 ,Accuracy: 0.32828572 , Cost: 1.5669442\n",
      "Current Step: 1480 ,Accuracy: 0.32828572 , Cost: 1.5669425\n",
      "Current Step: 1485 ,Accuracy: 0.32828572 , Cost: 1.5669425\n",
      "Current Step: 1490 ,Accuracy: 0.32828572 , Cost: 1.5669425\n",
      "Current Step: 1495 ,Accuracy: 0.32828572 , Cost: 1.5669427\n",
      "\n",
      "-------------CNN Trained------------------\n",
      "\n",
      "\n",
      "Shape of CNN output to be passed to AE (28000, 120)\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:152: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:161: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  Data loading\n",
    "feature = sc.loadmat(\"./S1_nolabel6.mat\")\n",
    "all = feature['S1_nolabel6']\n",
    "\n",
    "print (all.shape)\n",
    "\n",
    "\n",
    "# Shuffle data for better training\n",
    "np.random.shuffle(all)   \n",
    "\n",
    "final=2800*10 #Total number of datapoints to take\n",
    "all=all[0:final]\n",
    "feature_all =all[:,0:64]\n",
    "label=all[:,64:65]\n",
    "print(label)\n",
    "\n",
    "#Preprocessing\n",
    "feature_all=preprocessing.scale(feature_all)\n",
    "no_fea=feature_all.shape[-1]\n",
    "print(no_fea)\n",
    "label_all=one_hot(label)\n",
    "n_classes=6\n",
    "\n",
    "\n",
    "\n",
    "# -------Code for CNN------- #\n",
    "\n",
    "feature_all=feature_all# the input data of CNN\n",
    "print(\"Input to the CNN is of shape\", feature_all.shape)\n",
    "n_fea=feature_all.shape[-1]\n",
    "\n",
    "# Split data into train and test 3/4 and 1/4\n",
    "final=all.shape[0]\n",
    "middle_number=int(final*3/4)\n",
    "feature_training =feature_all[0:middle_number]\n",
    "feature_testing =feature_all[middle_number:final]\n",
    "label_training =label_all[0:middle_number]\n",
    "label_testing =label_all[middle_number:final]\n",
    "label_ww=label_all[middle_number:final]  # for the confusion matrix\n",
    "print (\"Number of labels: \",label_testing.shape)\n",
    "a = feature_training\n",
    "b = feature_testing\n",
    "\n",
    "keep=1\n",
    "batch_size=final-middle_number\n",
    "n_group=3\n",
    "train_fea=[]\n",
    "for i in range(n_group):\n",
    "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
    "    train_fea.append(f)\n",
    "\n",
    "\n",
    "train_label=[]\n",
    "for i in range(n_group):\n",
    "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
    "    train_label.append(f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Classification accuracy using CNN\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess3.run(prediction, feed_dict={xs: v_xs, keep_prob: keep})\n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess3.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: keep})\n",
    "    return result\n",
    "\n",
    "# Get random weights\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Get random biases\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Convolution\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "# 1*2 Pooling\n",
    "def max_pool_1x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,1,2,1], strides=[1,1,2,1], padding='SAME')\n",
    "\n",
    "cnn_out = tf.placeholder(tf.float32,[None,120])\n",
    "\n",
    "# definitions of inputs to network\n",
    "xs = tf.placeholder(tf.float32, [None, n_fea]) # Input Tensor\n",
    "ys = tf.placeholder(tf.float32, [None, n_classes])  # Output Tensor\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "x_image = tf.reshape(xs, [-1, 1, n_fea, 1]) # Convert into 4D tensor\n",
    " \n",
    "\n",
    "## conv1 layer ##\n",
    "W_conv1 = weight_variable([1,1, 1,2]) # patch 1*1, in size is 1 filter, out size is 2 filters\n",
    "#Wt_conv1 = weight_variable([1,20])\n",
    "b_conv1 = bias_variable([2])\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1) # output size 1*64*2\n",
    "h_pool1 = max_pool_1x2(h_conv1)                          # output size 1*32x2\n",
    "\n",
    "\n",
    "## conv2 layer ##\n",
    "W_conv2 = weight_variable([1,1,2,4]) # patch 1*1, in size 2, out size 4\n",
    "b_conv2 = bias_variable([4])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) # output size 1*32*4\n",
    "h_pool2 = max_pool_1x2(h_conv2)                          # output size 1*16*4\n",
    "\n",
    "\n",
    "## fc1 layer ##\n",
    "W_fc1 = weight_variable([int(1*(n_fea/4)*4),120])\n",
    "b_fc1 = bias_variable([120])\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, int(1*(n_fea/4)*4)])\n",
    "h_fc1 = tf.nn.sigmoid(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "## fc2 layer ##\n",
    "W_fc2 = weight_variable([120, n_classes])\n",
    "b_fc2 = bias_variable([n_classes])\n",
    "prediction = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "\n",
    "\n",
    "\n",
    "# the error between prediction and real data\n",
    "l2 = 0.001 * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=ys))+l2   # Softmax loss\n",
    "train_step = tf.train.AdamOptimizer(0.04).minimize(cross_entropy) # learning rate is 0.0001\n",
    "\n",
    "sess3 = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess3.run(init)\n",
    "\n",
    "print(\"-------------Training the CNN---------------\")\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "step = 1\n",
    "while step < 1500:\n",
    "    for i in range(n_group):\n",
    "        sess3.run(train_step, feed_dict={xs: train_fea[i], ys: train_label[i], keep_prob:keep})\n",
    "\n",
    "    if step % 5 == 0:\n",
    "        cost=sess3.run(cross_entropy, feed_dict={xs: b, ys: label_testing, keep_prob: keep})\n",
    "        acc_cnn_t=compute_accuracy(b, label_testing)\n",
    "        print('Current Step:',step,',Accuracy:',acc_cnn_t,', Cost:', cost)   \n",
    "    step+=1\n",
    "\n",
    "print(\"\\n-------------CNN Trained------------------\\n\\n\")\n",
    "acc_cnn=compute_accuracy(b, label_testing)\n",
    "time2=time.clock()\n",
    "\n",
    "\n",
    "#The following is the transformed 64->120 feature vector (fully connected layer)\n",
    "feature_all_cnn=sess3.run(h_fc1_drop, feed_dict={xs: feature_all, keep_prob: keep})\n",
    "\n",
    "\n",
    "print (\"Shape of CNN output to be passed to AE\",feature_all_cnn.shape)\n",
    "print(\"\\n\\n\")\n",
    "time3=time.clock()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "Tensor(\"ArgMax_600:0\", shape=(28000,), dtype=int64)\n",
      "(28000, 6)\n",
      "(21000, 1, 64)\n",
      "(7000, 1, 64)\n",
      "(7000, 1, 64)\n",
      "(7000, 6)\n",
      "\n",
      "\n",
      "\n",
      "-------------Training RNN---------------\n",
      ", The step is: 0 , The accuracy is: 0.30857143 The cost is : 43.141014\n",
      ", The step is: 5 , The accuracy is: 0.33657143 The cost is : 38.418587\n",
      ", The step is: 10 , The accuracy is: 0.32857144 The cost is : 34.505966\n",
      ", The step is: 15 , The accuracy is: 0.34257144 The cost is : 30.968946\n",
      ", The step is: 20 , The accuracy is: 0.331 The cost is : 27.815666\n",
      ", The step is: 25 , The accuracy is: 0.34571427 The cost is : 24.988352\n",
      ", The step is: 30 , The accuracy is: 0.36028573 The cost is : 22.465488\n",
      ", The step is: 35 , The accuracy is: 0.36685714 The cost is : 20.22836\n",
      ", The step is: 40 , The accuracy is: 0.369 The cost is : 18.242996\n",
      ", The step is: 45 , The accuracy is: 0.37685713 The cost is : 16.465904\n",
      ", The step is: 50 , The accuracy is: 0.41 The cost is : 14.856402\n",
      ", The step is: 55 , The accuracy is: 0.44142857 The cost is : 13.414605\n",
      ", The step is: 60 , The accuracy is: 0.48242858 The cost is : 12.107275\n",
      ", The step is: 65 , The accuracy is: 0.51428574 The cost is : 10.955413\n",
      ", The step is: 70 , The accuracy is: 0.5372857 The cost is : 9.911219\n",
      ", The step is: 75 , The accuracy is: 0.59585714 The cost is : 8.948522\n",
      ", The step is: 80 , The accuracy is: 0.6405714 The cost is : 8.090318\n",
      ", The step is: 85 , The accuracy is: 0.6852857 The cost is : 7.314993\n",
      ", The step is: 90 , The accuracy is: 0.73085713 The cost is : 6.6152906\n",
      ", The step is: 95 , The accuracy is: 0.7598571 The cost is : 6.030566\n",
      ", The step is: 100 , The accuracy is: 0.79585713 The cost is : 5.4956894\n",
      ", The step is: 105 , The accuracy is: 0.82757145 The cost is : 5.025086\n",
      ", The step is: 110 , The accuracy is: 0.85342854 The cost is : 4.597297\n",
      ", The step is: 115 , The accuracy is: 0.88014287 The cost is : 4.2132215\n",
      ", The step is: 120 , The accuracy is: 0.9004286 The cost is : 3.8795881\n",
      ", The step is: 125 , The accuracy is: 0.91985714 The cost is : 3.5684688\n",
      ", The step is: 130 , The accuracy is: 0.9244286 The cost is : 3.3150902\n",
      ", The step is: 135 , The accuracy is: 0.9315714 The cost is : 3.0860624\n",
      ", The step is: 140 , The accuracy is: 0.94214284 The cost is : 2.8711307\n",
      ", The step is: 145 , The accuracy is: 0.9484286 The cost is : 2.682672\n",
      ", The step is: 150 , The accuracy is: 0.95 The cost is : 2.5270147\n",
      ", The step is: 155 , The accuracy is: 0.9531429 The cost is : 2.3768666\n",
      ", The step is: 160 , The accuracy is: 0.95928574 The cost is : 2.2378397\n",
      ", The step is: 165 , The accuracy is: 0.95271426 The cost is : 2.1337643\n",
      ", The step is: 170 , The accuracy is: 0.95685714 The cost is : 2.0169375\n",
      ", The step is: 175 , The accuracy is: 0.9538571 The cost is : 1.9303176\n",
      "The lambda is : 0.004 , Learning rate: 0.005 , The step is: 179 , The accuracy is:  0.96014285\n",
      "-------------------RNN Trained--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:174: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Confusion Matrix after training the RNN\n",
      "[[   0    0    0    0    0    0]\n",
      " [   0 2249   19   16   10    4]\n",
      " [   0   50 1177   11    8    8]\n",
      " [   0   22   12 1081    5    6]\n",
      " [   0   13   27   10 1041    7]\n",
      " [   0   30   14    4    3 1173]]\n",
      "RNN train time: 410.35572 Rnn test time 1.1886859999999615 RNN total time 411.544406\n",
      "Shapes of the two networks: \n",
      "Shapes of the RNN and CNN outputs respectively are: \n",
      "(28000, 64) (28000, 120)\n",
      "Now we combine the two outputs to one and train the AutoEncoder\n",
      "\n",
      "Shape of the input to the the autoencoder: \n",
      "(28000, 184)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:190: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## ----Code for RNN----- ##\n",
    "feature_all=feature_all\n",
    "no_fea=feature_all.shape[-1]\n",
    "print(no_fea)\n",
    "feature_all =feature_all.reshape([final,1,no_fea])\n",
    "print(tf.argmax(label_all,1))\n",
    "\n",
    "\n",
    "print(label_all.shape)\n",
    "\n",
    "# Train and test data split\n",
    "# middle_number=21000\n",
    "feature_training =feature_all[0:middle_number]\n",
    "feature_testing =feature_all[middle_number:final]\n",
    "label_training =label_all[0:middle_number]\n",
    "label_testing =label_all[middle_number:final]\n",
    "a=feature_training\n",
    "b=feature_testing\n",
    "print(feature_training.shape)\n",
    "print(feature_testing.shape)\n",
    "nodes=64\n",
    "lameda=0.004\n",
    "lr=0.005\n",
    "\n",
    "# Batches for training\n",
    "batch_size=final-middle_number\n",
    "train_fea=[]\n",
    "n_group=3\n",
    "for i in range(n_group):\n",
    "    f =a[(0+batch_size*i):(batch_size+batch_size*i)]\n",
    "    train_fea.append(f)\n",
    "print (train_fea[0].shape)\n",
    "\n",
    "train_label=[]\n",
    "for i in range(n_group):\n",
    "    f =label_training[(0+batch_size*i):(batch_size+batch_size*i), :]\n",
    "    train_label.append(f)\n",
    "print (train_label[0].shape)\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "n_inputs = no_fea\n",
    "n_steps = 1 # time steps\n",
    "n_hidden1_units = nodes   # neurons in hidden layer\n",
    "n_hidden2_units = nodes\n",
    "n_hidden3_units = nodes\n",
    "n_hidden4_units=nodes\n",
    "n_classes = n_classes\n",
    "\n",
    "# tf Graph input\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Define weights\n",
    "\n",
    "weights = {\n",
    "'in': tf.Variable(tf.random_normal([n_inputs, n_hidden1_units]), trainable=True),\n",
    "'a': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden1_units]), trainable=True),\n",
    "'hidd2': tf.Variable(tf.random_normal([n_hidden1_units, n_hidden2_units])),\n",
    "'hidd3': tf.Variable(tf.random_normal([n_hidden2_units, n_hidden3_units])),\n",
    "'hidd4': tf.Variable(tf.random_normal([n_hidden3_units, n_hidden4_units])),\n",
    "'out': tf.Variable(tf.random_normal([n_hidden4_units, n_classes]), trainable=True),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "'in': tf.Variable(tf.constant(0.1, shape=[n_hidden1_units])),\n",
    "'hidd2': tf.Variable(tf.constant(0.1, shape=[n_hidden2_units ])),\n",
    "'hidd3': tf.Variable(tf.constant(0.1, shape=[n_hidden3_units])),\n",
    "'hidd4': tf.Variable(tf.constant(0.1, shape=[n_hidden4_units])),\n",
    "'out': tf.Variable(tf.constant(0.1, shape=[n_classes ]), trainable=True)\n",
    "}\n",
    "\n",
    "\n",
    "def RNN(X, weights, biases):\n",
    "\n",
    "    # transpose the inputs shape from\n",
    "    X = tf.reshape(X, [-1, n_inputs])\n",
    "\n",
    "    # into hidden\n",
    "    X_hidd1 = tf.matmul(X, weights['in']) + biases['in']\n",
    "    X_hidd2 = tf.matmul(X_hidd1, weights['hidd2']) + biases['hidd2']\n",
    "    X_hidd3 = tf.matmul(X_hidd2, weights['hidd3']) + biases['hidd3']\n",
    "    X_hidd4 = tf.matmul(X_hidd3, weights['hidd4']) + biases['hidd4']\n",
    "    X_in = tf.reshape(X_hidd4, [-1, n_steps, n_hidden4_units])\n",
    "\n",
    "\n",
    "    # basic LSTM Cell.\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden4_units, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    # lstm cell is divided into two parts (c_state, h_state)\n",
    "    init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "\n",
    "    with tf.variable_scope('lstm1'):\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "\n",
    "    # hidden layer for output as the final results\n",
    "    #############################################\n",
    "    outputs = tf.unstack(tf.transpose(outputs, [1, 0, 2]))    # states is the last outputs\n",
    "    results = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "    return results, outputs[-1]\n",
    "\n",
    "pred,Feature = RNN(x, weights, biases)\n",
    "\n",
    "# Merge outputs of CNN and RNN\n",
    "#merger = tf.concat(Feature, prediction)\n",
    "#a = merger\n",
    "\n",
    "\n",
    "\n",
    "lamena =lameda\n",
    "l2 = lamena * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())  # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y)) + l2  # Softmax loss\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "\n",
    "    # train_op = tf.train.RMSPropOptimizer(0.00001).minimize(cost)\n",
    "\n",
    "# pred_result =tf.argmax(pred, 1)\n",
    "label_true =tf.argmax(y, 1)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "confusion_m=tf.confusion_matrix(tf.argmax(y, 1), tf.argmax(pred, 1))\n",
    "\n",
    "print(\"\\n\\n\\n-------------Training RNN---------------\")\n",
    "with tf.Session() as sess:\n",
    "    if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    #saver = tf.train.Saver()\n",
    "    step = 0\n",
    "\n",
    "    filename = \"./rnn_out.csv\"\n",
    "    f2 = open(filename, 'wb')\n",
    "\n",
    "    while step < 2500:\n",
    "        for i in range(n_group):\n",
    "            sess.run(train_op, feed_dict={\n",
    "                x: train_fea[i],\n",
    "                y: train_label[i],\n",
    "            })\n",
    "        if sess.run(accuracy, feed_dict={x: b,y: label_testing,})>0.96:\n",
    "            print(\n",
    "            \"The lambda is :\", lamena, \", Learning rate:\", lr, \", The step is:\", step, \", The accuracy is: \",\n",
    "            sess.run(accuracy, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            }))\n",
    "            break\n",
    "\n",
    "\n",
    "        if step % 5 == 0:\n",
    "            hh=sess.run(accuracy, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            })\n",
    "            #f2.write(str(hh)+'\\n')\n",
    "\n",
    "            print(\", The step is:\",step,\", The accuracy is:\", hh, \"The cost is :\",sess.run(cost, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            }))\n",
    "        step += 1\n",
    "    #a=feature_training\n",
    "    ##confusion matrix\n",
    "\n",
    "    print(\"-------------------RNN Trained--------------------\")\n",
    "\n",
    "    time4 = time.clock()\n",
    "    feature_0=sess.run(Feature, feed_dict={x: train_fea[0]})\n",
    "    for i in range(1,n_group):\n",
    "        feature_11=sess.run(Feature, feed_dict={x: train_fea[i]})\n",
    "        feature_0=np.vstack((feature_0,feature_11))\n",
    "        \n",
    "\n",
    "    feature_b = sess.run(Feature, feed_dict={x: b})\n",
    "    feature_all_rnn=np.vstack((feature_0,feature_b))\n",
    "\n",
    "    confusion_m=sess.run(confusion_m, feed_dict={\n",
    "                x: b,\n",
    "                y: label_testing,\n",
    "            })\n",
    "    print(\"\\n\\n\\nConfusion Matrix after training the RNN\")\n",
    "    print(confusion_m)\n",
    "    time5 = time.clock()\n",
    "\n",
    "    print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
    "\n",
    "#Reinitialize variables\n",
    "\n",
    "#Shapes of the RNN and CNN outputs\n",
    "\n",
    "print(\"Shapes of the two networks: \")\n",
    "print(\"Shapes of the RNN and CNN outputs respectively are: \")\n",
    "print(feature_all_rnn.shape, feature_all_cnn.shape)\n",
    "\n",
    "\n",
    "feature_all=np.hstack((feature_all_rnn,feature_all_cnn))\n",
    "no_fea=feature_all.shape[-1]\n",
    "print(\"Now we combine the two outputs to one and train the AutoEncoder\\n\")\n",
    "print(\"Shape of the input to the the autoencoder: \")\n",
    "print(feature_all.shape)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "# Reprocess as the number of features have changed\n",
    "# middle_number=21000\n",
    "feature_training =feature_all[0:middle_number]\n",
    "feature_testing =feature_all[middle_number:final]\n",
    "label_training =label_all[0:middle_number]\n",
    "label_testing =label_all[middle_number:final]\n",
    "a=feature_training\n",
    "b=feature_testing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------Training the autoencoder--------------------\n",
      "\n",
      "Epoch: 0001 cost= 0.326481193\n",
      "Epoch: 0011 cost= 0.295376152\n",
      "Epoch: 0021 cost= 0.244253457\n",
      "Epoch: 0031 cost= 0.221476853\n",
      "Epoch: 0041 cost= 0.225512117\n",
      "Epoch: 0051 cost= 0.232943594\n",
      "Epoch: 0061 cost= 0.227262810\n",
      "Epoch: 0071 cost= 0.219803736\n",
      "Epoch: 0081 cost= 0.219978943\n",
      "Epoch: 0091 cost= 0.223190382\n",
      "Epoch: 0101 cost= 0.223190382\n",
      "Epoch: 0111 cost= 0.223193392\n",
      "Epoch: 0121 cost= 0.223139361\n",
      "Epoch: 0131 cost= 0.223139361\n",
      "Epoch: 0141 cost= 0.223010704\n",
      "Epoch: 0151 cost= 0.223187268\n",
      "Epoch: 0161 cost= 0.223184034\n",
      "Epoch: 0171 cost= 0.223129794\n",
      "Epoch: 0181 cost= 0.223081946\n",
      "Epoch: 0191 cost= 0.222652823\n",
      "Epoch: 0201 cost= 0.223029450\n",
      "Epoch: 0211 cost= 0.227058321\n",
      "Epoch: 0221 cost= 0.231090561\n",
      "Epoch: 0231 cost= 0.231090561\n",
      "Epoch: 0241 cost= 0.231090561\n",
      "Epoch: 0251 cost= 0.231090561\n",
      "Epoch: 0261 cost= 0.231090546\n",
      "Epoch: 0271 cost= 0.231090546\n",
      "Epoch: 0281 cost= 0.231090546\n",
      "Epoch: 0291 cost= 0.231090546\n",
      "Epoch: 0301 cost= 0.231090546\n",
      "Epoch: 0311 cost= 0.231090546\n",
      "Epoch: 0321 cost= 0.231090546\n",
      "Epoch: 0331 cost= 0.231090546\n",
      "Epoch: 0341 cost= 0.231090546\n",
      "Epoch: 0351 cost= 0.231090546\n",
      "Epoch: 0361 cost= 0.231090546\n",
      "Epoch: 0371 cost= 0.231090546\n",
      "Epoch: 0381 cost= 0.231090546\n",
      "Epoch: 0391 cost= 0.231090546\n",
      "Optimization Finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:78: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Autoencoder trained\n",
      "\n",
      "AE train time: 4895.286797999999 AE test time 0.9945040000002336 AE total time 4896.281301999999\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/achal/virtualenvs/pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:83: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##AE\n",
    "feature_all=feature_all\n",
    "train_fea=feature_all[0:middle_number]\n",
    "\n",
    "group=3\n",
    "display_step = 10\n",
    "training_epochs = 400\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 800  \n",
    "n_hidden_2=100\n",
    "n_input_ae = no_fea \n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, n_input_ae])\n",
    "weights_ae = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input_ae, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input_ae])),\n",
    "}\n",
    "biases_ae = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input_ae])),\n",
    "}\n",
    "\n",
    "\n",
    "# Output of the encoder\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights_ae['encoder_h1']),\n",
    "                                   biases_ae['encoder_b1']))\n",
    "    return layer_1\n",
    "\n",
    "\n",
    "# Output of the decoder\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights_ae['decoder_h2']),\n",
    "                                   biases_ae['decoder_b2']))\n",
    "    return layer_1\n",
    "\n",
    "print(\"\\n----------------Training the autoencoder--------------------\\n\")\n",
    "for ll in range(1):\n",
    "    learning_rate = 0.2\n",
    "    for ee in range(1):\n",
    "        # Construct model\n",
    "        encoder_op = encoder(X)\n",
    "        decoder_op = decoder(encoder_op)\n",
    "        # Prediction\n",
    "        y_pred = decoder_op\n",
    "        # Targets (Labels) are the input data.\n",
    "        y_true = X\n",
    "\n",
    "        cost = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "     \n",
    "        # Launch the graph\n",
    "        with tf.Session() as sess1:\n",
    "            sess1.run(init)\n",
    "            #saver = tf.train.Saver()\n",
    "            # Training cycle\n",
    "            for epoch in range(training_epochs):\n",
    "                # Loop over all batches\n",
    "                for i in range(group):\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, c = sess1.run([optimizer, cost], feed_dict={X: a})\n",
    "                    \n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                          \"cost=\", \"{:.9f}\".format(c))\n",
    "            print(\"Optimization Finished!\")\n",
    "            time6=time.clock()\n",
    "            a = sess1.run(encoder_op, feed_dict={X: a})\n",
    "            \n",
    "            b = sess1.run(encoder_op, feed_dict={X: b})\n",
    "print(\"\\n\\nAutoencoder trained\\n\")\n",
    "time7=time.clock()\n",
    "print (\"AE train time:\", time6 - time5, \"AE test time\", time7 - time6, 'AE total time', time7 - time5)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-a221986c2b25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m##XGBoost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mxg_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mxg_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_testing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "\n",
    "##XGBoost\n",
    "import xgboost as xgb\n",
    "xg_train = xgb.DMatrix(a, label=np.argmax(label_training,1))\n",
    "xg_test = xgb.DMatrix(b, label=np.argmax(label_testing,1))\n",
    "\n",
    "# Set params for XGBoost \n",
    "param = {}\n",
    "# Softmax for multi-class\n",
    "param['objective'] = 'multi:softprob'\n",
    "# It will give probability for each class\n",
    "# Scale weight of positive examples\n",
    "param['eta'] = 0.5\n",
    "param['max_depth'] = 6\n",
    "param['silent'] = 1\n",
    "param['nthread'] = 4\n",
    "param['subsample']=0.9\n",
    "param['num_class'] =n_classes\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "watchlist = [ (xg_train,'train'), (xg_test, 'test') ]\n",
    "num_round = 500\n",
    "bst = xgb.train(param, xg_train, num_round, watchlist );\n",
    "time8=time.clock()\n",
    "pred = bst.predict(xg_test);\n",
    "\n",
    "num_test = 7000\n",
    "num_correct = 0\n",
    "for i in range(len(pred)):\n",
    "    maxi = 0\n",
    "    maxindex = 0\n",
    "    for j in range(len(pred[i])):\n",
    "        if(maxi<pred[i][j]):\n",
    "            maxi = pred[i][j]\n",
    "            maxindex = j\n",
    "    predIndex = 0\n",
    "    for j in range(6):\n",
    "        if(label_testing[i][j]==1):\n",
    "            predIndex = j\n",
    "    \n",
    "    if(predIndex==maxindex):\n",
    "        num_correct += 1\n",
    "\n",
    "    \n",
    "print(\"Testing Results: \")\n",
    "print(\"Number of correct predictions: \",num_correct)\n",
    "print(\"Accuracy: \",(num_correct/7000.0))\n",
    "\n",
    "#\n",
    "#print ('predicting, classification error=%f' %(sum( int(pred[i]) != label_testing[i] for i in range(len(label_testing))) / float(len(label_testing)) ))\n",
    "#time9=time.clock()\n",
    "\n",
    "\n",
    "# print (\"CNN train time:\", time2-time1, \"cnn test time\", time3-time2, 'CNN total time', time3-time1)\n",
    "# print (\"RNN train time:\", time4 - time3, \"Rnn test time\", time5 - time4, 'RNN total time', time5 - time3)\n",
    "# print (\"AE train time:\", time6 - time5, \"AE test time\", time7 - time6, 'AE total time', time7 - time5)\n",
    "# print (\"XGB train time:\", time8 - time7, \"XGB test time\", time9 - time8, 'XGB total time', time9 - time7)\n",
    "# print 'total train time', time2-time1+time4 - time3+time6 - time5+time8 - time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
